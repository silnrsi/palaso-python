#!/usr/bin/env python
'''
Convert the publishable vernacular text in a USFM file according to a given
.tec mapping file. In addition, support dictionary replacement, and automatic
case generation.
'''
__version__ = '0.1.1'
__date__    = '11 April 2011'
__author__  = 'Martin Hosken <martin_hosken@sil.org>'
__credits__ = '''\
Tim Eves provided the concordance program on which this is based.
'''

from palaso.sfm import usfm, style, pprint, element, text
from palaso.teckit.engine import Converter, Mapping
import palaso.unicsv as csv
import codecs, optparse, os.path, re, warnings, sys
import unicodedata
from itertools import groupby

class scrparser(usfm.parser) :
    def _canonicalise_footnote(self, content) :
        return content


word_cat = set(['Lu', 'Ll', 'Lt', 'Lm', 'Lo', 'Mn', 'Mc', 'Me', 'Pd', 'Cs', 'Co', 'Cn'])
isletters = ""

def sfmmap(elements, elemente, textf, doc) :
    def _g(e) :
        if isinstance(e,element) :
            e = elements(e)
            e[0:] = map(_g, e)
            elemente(e)
            return e
        else :
            e_ = textf(e)
            return text(e_, e.pos, e)
    return map(_g, doc)

def isword(char) :
    if char in isletters : return True
    return unicodedata.category(char) in word_cat

def aswords(txt) :
    ''' returns array of word forming, punc, word forming, punc '''
    g = [(i[0], "".join(i[1])) for i in groupby(txt, isword)]
    res = [i[1] for i in g]
    if not g[0][0] : res.insert(0, "")
    return res

class notec(object) :
    def convert(self, txt, **kw) : return txt

class usfm_transducer(object) :

    def __init__(self, case = '', tec = notec(), opts = {}) :
        self.caps = [1]
        self.oquotes = opts.openquotes.decode('raw_unicode_escape')
        self.cquotes = opts.closequotes.decode('raw_unicode_escape')
        self.punc = opts.sentencepunc.decode('raw_unicode_escape')
        self.capquotes = opts.capitalopenquotes.decode('raw_unicode_escape') if opts.capitalopenquotes else self.oquotes
        self.case = case
        self.enc = tec
        self.opts = opts
        self.tags = {}
        self.dict = {}
        self.phrases = {}
        self.phrase_keys = {}
        self.morphs = {}
        self.morph_keys = ''
        self.morphid = '!'
        if opts.capitaltags :
            self.ctags = set(opts.capitaltags.split())
        else :
            self.ctags = set()
        if opts.normalize :
            self.normal = 'NF' + opts.normalize.upper()
        else :
            self.normal = None

    def load_dict(self, fname, incol, outcol, tagcol = None) :
        morph_keys = []
        fh = open(fname, 'rb')
        entries = csv.reader(fh, skipinitialspace=True)
        for e in entries :
            if not len(e) : continue
            k = self.enc.convert(e[incol], finished=True).strip()
            if k.find(self.morphid) != -1 :     # has stem marker
                if k[0] != self.morphid :
                    mk = '^' + k
                else :
                    mk = k[1:]
                    k = k[1:]
                if k[-1] != self.morphid :
                    mk = mk + '$'
                else :
                    k = k[0:-1]
                    mk = mk[0:-1]
                morph_keys.append(mk)
                self.morphs[k] = e[outcol].strip()
            elif len(filter(isword, k)) != len(k) :
                self.phrases[k] = e[outcol].strip()
            else :                              # dictionary entry
                self.dict[k] = e[outcol].strip()
            if tagcol != None and e[tagcol] :                      # has tag constraints
                self.tags[k] = set(e[tagcol].split())
        morph_keys.sort(cmp = lambda x, y : cmp(len(y), len(x)))
        self.morph_keys = "|".join(morph_keys)
        phrase_keys = sorted(self.phrases.keys(), cmp = lambda x, y: cmp(len(y), len(x)))
        for k in phrase_keys :
            p = aswords(k)
            if p[0] in self.phrase_keys :
                self.phrase_keys[p[0]].append(p)
            else :
                self.phrase_keys[p[0]] = [p]
        fh.close()

    def element_start(self, node) :
        try :
            if node.meta['StyleType'] == 'Note' :
                self.caps.append(1)
        except KeyError as e:
            e.args = ("Element: %s at %r" % (node.name, node.pos), )
            raise e
        return node
    
    def element_end(self, node) :
        try :
            if node.meta['StyleType'] == 'Note' :
                self.caps.pop()
        except KeyError as e:
            e.args = ("Element: %s at %r" % (node.name, node.pos), )
            raise e

    def convert_node(self, tnode) :
        if not (set(tnode.parent.meta['TextProperties']) & set(('publishable', 'vernacular'))) :
            return tnode
        if (tnode.parent.meta['StyleType'] == 'Paragraph' and tnode is tnode.parent[0]) or tnode.parent.name in self.ctags :
            self.caps[-1] = 1
        self.tnode = tnode
        res = re.sub(ur'[^\x00-\x1f]+', self.convert, tnode)
        if self.normal : res = unicodedata.normalize(self.normal, res)
        return res

    def convert(self, match) :
        if self.opts.binary :
            input = match.group(0).encode('latin_1')
        else :
            input = match.group(0)
        res = self.enc.convert(input, finished=True)
        if res.strip() == '' : return res
        if self.caps[-1] :
            res = re.sub(ur'^([\s' + self.oquotes + ur']*)(.)',
                            lambda m : m.group(1) + m.group(2).upper(), res)
        if re.search(ur'[{1}][{0}]*\s*$'.format(self.cquotes, self.punc), res) :
            self.caps[-1] = 1
        elif re.match(ur'^\s*\S', res) :
            self.caps[-1] = 0
        rlist = aswords(res)
        res = ""
        while len(rlist) :
            l = len(rlist)
            wd = rlist.pop(0)
            if wd :
                case = wd[0].isupper()
                wd = wd[0].lower() + wd[1:]
                found = False
                if wd in self.phrase_keys :
                    for p in self.phrase_keys[wd] :
                        if len(p) <= l and p[1:] == rlist[0:len(p)-1] :
                            s = "".join(p)
                            if s in self.tags and self.tnode.parent.name not in self.tags[s] : continue
                            del rlist[0:len(p)-1]
                            wd = self.phrases[s]
                            found = True
                            break
                if not found and wd in self.dict and (not wd in self.tags or self.tnode.parent.name in self.tags[wd]) :
                    wd = self.dict[wd]
                    found = True
                elif self.morph_keys and not found :
                    wd = re.sub('((?iu)' + self.morph_keys + ')', 
                                lambda m : self.morphs[m.group(1)] if m.group(1) not in self.tags or self.tnode.parent.name in self.tags[wd] else m.group(1), wd)
                if case :
                    wd = wd[0].upper() + wd[1:]
            if len(rlist) :
                res += wd + rlist.pop(0)
            else :
                res += wd

        res = re.sub(ur'([{0}][{1}\s]*[{2}\s]*)(.)'.format(self.punc, self.cquotes, self.oquotes), lambda m : m.group(1) + (m.group(2).upper() if isword(m.group(2)[0]) else m.group(2)), res)

        if self.tnode.parent.meta['TextType'] == 'Title' and self.capquotes:
            res = re.sub(ur'(^\s*|\s)([' + self.capquotes + ur']*)(.)', lambda m : m.group(1) + m.group(2) + (m.group(3).upper() if isword(m.group(3)[0]) else m.group(3)), res)
        elif self.capquotes :
            res = re.sub('(?<=[' + self.capquotes + ur'])([' + self.oquotes + ']*)(.)', lambda m: m.group(1) + m.group(2).upper() if isword(m.group(2)[0]) else m.group(1) + m.group(2), res)

        if self.case :
            res = re.sub(self.case, lambda m : m.group(0)[0].upper() + m.group(0)[1:], res)
        return res

def transduce(fname, opts) :
    def identity_mkr(*args) : return args

    conv = usfm_transducer(case=opts.capitalise, opts=opts)
    if opts.tec :
        tec = Converter(Mapping(opts.tec), forward = not opts.reverse)
        conv.enc = tec
    if opts.dict :
        conv.load_dict(opts.dict, opts.dictinput, opts.dictoutput, opts.dicttag)
    infh = codecs.open(fname, 'r', 'latin_1' if opts.binary else 'utf_8_sig')
    try:
       doc = sfmmap(conv.element_start, conv.element_end, conv.convert_node, scrparser(infh, stylesheet=opts.stylesheet, private=opts.strict))
    except SyntaxError, err :
        sys.stderr.write(parser.expand_prog_name('%prog: failed to parse USFM: {0!s}\n').format(err))
        sys.exit(1)
    finally:
        infh.close()
    return doc


if __name__ == '__main__':
    parser = optparse.OptionParser(usage='%prog [options] <SFM FILE>\n' + __doc__)
    parser.set_defaults(sentencepunc=".!?",
        openquotes="'\"\u2018\u201C\[{(<\u00AB",
        closequotes="'\"\u2019\u201D\]})>\u00BB")
    parser.add_option("-b","--binary",action="store_true",help="Input is legacy encoded, not Unicode")
    parser.add_option("-c","--capitalise",action="store",help="regular expression match which is capitalised")
    parser.add_option("--capitaltags",action="store",help="list of tags to capitalise their contents")
    parser.add_option("--openquotes",action="store",help="list of opening quote chars as a string with \u escaping [default: %default]")
    parser.add_option("--closequotes",action="store",help="list of closing quote chars as a string with \u escaping [default: %default]")
    parser.add_option("--sentencepunc",action="store",help="list of sentence final punctuation with \u escaping [default: %default]")
    parser.add_option("--capitalopenquotes",action="store",help="list of chars that force a new sentence [default value of --openquotes]")
    parser.add_option("--letters",action="store",help="more word forming characters, with \u escaping")
    parser.add_option("-d","--dict",action="store",help="CSV dictionary of input output replacements")
    parser.add_option("--dict-input",action="store",type="int",dest="dictinput",default=0,help="Input column of CSV dictionary [%default]")
    parser.add_option("--dict-output",action="store",type="int",default=1,dest="dictoutput",help="Output column of CSV dictionary [%default]")
    parser.add_option("--dict-context",action="store",type="int",dest="dicttag",help="Marker context column of CSV dictionary")
    parser.add_option("-n","--lines",action="store_true",help="Don't parse as USFM, just as plain text")
    parser.add_option("-o","--output",action="store",help="Specify output file or directory")
    parser.add_option("-t","--tec",action="store",help="TECKit .tec file to use for conversion")
    parser.add_option("-r","--reverse",action="store_true",help="Run .tec file in reverse")
    parser.add_option("--normalize",action="store",help="Unicode normalize converted text [c,d,kc,kd]")
    parser.add_option("-v","--verbose",action='store_true',default=False,
                      help='Print out statistics and progress info')
    parser.add_option("-w","--warnings",action='store_true',default=True,
                      help='Print out syntax warnings discovered during SFM parsing')
    parser.add_option("-s","--strict",action='store_true',default=False,
                      help='Turn on strict parsing mode. Markers not in the stylesheet or private name space will cause an error')
    parser.add_option("-S","--stylesheet",action='store',type='string',
                     metavar='PATH', default=None,
                     help='User stylesheet to add/override marker definitions to the default USFM stylesheet')
    parser.add_option("-V","--version",action="store_true",help="Print program version and exit")

    opts,sfms = parser.parse_args()

    if opts.version :
        sys.stdout.write(__version__+"\n")
        sys.exit(0)

    if len(sfms) < 1:
        sys.stderr.write(parser.expand_prog_name('%prog: missing SFM FILE\n'))
        parser.print_help(file=sys.stderr)
        sys.exit(1)
    
    opts.warnings = opts.strict or opts.warnings
    if opts.stylesheet:
        stylesheet_path = opts.stylesheet
        opts.stylesheet = usfm.default_stylesheet.copy()
        opts.stylesheet.update(style.parse(open(stylesheet_path,'r')))
    else:
        opts.stylesheet = usfm.default_stylesheet

    if opts.letters : isletters = opts.letters.decode('raw_unicode_escape')

    work = []
    first_def = -1
    if not opts.output :
        first_def = 0
    elif not os.path.isdir(opts.output) :
        work.append((sfms[0],opts.output))
        first_def = 1
    else :
        work.extend(zip(sfms, map(lambda x: os.path.join(opts.stdout, os.path.split(x)[1]), sfms)))
    if first_def > -1 :
        work.extend(zip(sfms[first_def:], map(lambda x: x+"_u", sfms[first_def:])))

    try:
        with warnings.catch_warnings():
            warnings.simplefilter("always" if opts.warnings else "ignore", SyntaxWarning)
            for job in work :
                res = pprint(transduce(job[0], opts))
                ofh = codecs.open(job[1], "w", "utf-8")
                ofh.write(res)
                ofh.close()
                
    except IOError, err:
        sys.stderr.write(parser.expand_prog_name('%prog: IO error: {0!s}\n').format(err))
        sys.exit(2)

